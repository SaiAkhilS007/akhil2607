# What exactly is big data?

Big data is data that contains greater variety, arriving in increasing volumes and with more velocity. This is also known as the three Vs.

In common words, Big data contains of large, more complex data sets, especially from new data sources from all types of media these days. These data sets are so voluminous that traditional data processing software like RDMS just can’t manage them. But these massive volumes of data can be used to address business problems you wouldn’t have been able to tackle before.

Some of the famous examples of Big Data are **Healthcare, Transportation, Smart Cities Infrastructure, Sports, Media and Entertainment.**

### Types of Big Data

<img src="https://github.com/akhil2607/akhil2607/assets/44285279/e2c07e42-80a8-42dc-9f0d-36e86b19775b" width="500" height="200">

#### Structured Data
Structured data is stored in a record's set field. We utilize this information everyday. like your name and place. It's schemas connect all data, so it has the same features. You may call structured data as linked data. For data consistency, it is separated across many tables with one record per object. Table limitations ensure relationship success. Structured data helps firms maximize their data analysis tools and methodologies. 
Examples are **Databases such as Oracle, MySQL and Spreadsheets.**

#### Semi-Structured Data
It is also called as NoSQL data because it is not bound to any RDMS schemas for data storage or handling. It is also not organized into raws and columns like spreadsheets. This contains the information which is came from the external resources such as social meida platforms like facebooks or web-based data feeds like twitter or news channels. A data serialization language is used to exchange semi-structured data across systems and also store metadata about a business process but it can also include files containing machine instructions for computer programs.
Examples are **E-mails, Audio, Zip Files, XML and JSON.**

#### Unstructured Data
Unstructured data lacks a schema or rules. Its setup is random. Unstructured data includes photos, videos, text, and log files. The data behind a picture or video is unstructured, even if the metadata is semi-structured. Without the right software, unstructured data is called “dark data” since it cannot be examined.                                                                                                               
Examples are **Videos, Text Messages, Chats, Webpages and images.**

# 6V’s of Big Data
<img src="https://github.com/akhil2607/akhil2607/assets/44285279/106edaca-b287-4cc7-b078-b4203a071c5d" width="500" height="200">

The Characteristics of Big Data are defined in form of “6Vs” :

### 1. Volume:
Just the word "Big Data" makes you think of something very big. There is a lot of info in volume. To figure out how much info is worth, its size is very important. "Big Data" refers to very large amounts of data. And this means that the amount of data determines whether it can really be called "Big Data" or not. So, when working with "Big Data," it's important to think about a typical "Volume." Big data is a type of data that is so big that it can't fit on a single computer. To keep, handle, and examine it, you need special tools and systems.

**Example:** In the year 2016, the estimated global mobile traffic was 6.2 Exabytes (6.2 billion GB) per month. Also, by the year 2020 we will have almost 40000 Exabytes of data. And now in 2023 we have 120 zettabytes. 

### 2. Velocity:
It is very fast for info to build up. This is called velocity. When you turn on Big Data velocity, data comes in from places like computers, networks, social networks, cell phones, and more. The rush of info is huge and never stops. This shows how much data can be used and how quickly it can be created and handled to meet needs. Picking out pieces of data can help you solve problems like "velocity."

**Example:** The number of searches on Google every day is over 3.5 billion. Also, the number of Facebook users grows by about 22% every year.

### 3. Variety:
The types of data that it refers to are organized, semi-structured, and random. It can also mean different kinds of sources. Different kinds of data coming in from inside and outside of a business are called variety. It can be organized, partly organized, or not organized at all.

**Example:** The high variety data sets would be the CCTV audio and video files that are generated at various locations in a city. 

### 4. Veracity:
It refers to data that isn't always consistent and isn't always clear. In other words, current data can get jumbled and it's hard to keep an eye on its quality and accuracy. Another thing that makes Big Data unpredictable is the large amount of different data factors that come from different data types and sources.

**Example:** A lot of data could be confusing, while not enough data could only give you half of the information you need.

### 5. Value:
After you think about the first four Vs, there is one more V that stands for Value! Most data that doesn't have any value is useless to the business unless it is turned into something useful. Just having data isn't useful or important; it needs to be changed into something useful in order to get information. This means that Value! is the most important V out of the six.

**Example:** Lead data and demographics. Website and product usage data. Customer purchase data.

### 6. Variability: 
How quickly or easily can you get data that changes the way your data is structured? How often does your data change what it means or how it looks?. The number of inconsistencies in the data. Variability can also refer to the inconsistent speed at which big data is loaded into your database.

**Example:**  For instance, consider a restaurant menu that comprises three items. The number of unique items is the variety, but variability is when you order the same item on the menu and it tastes different every time you order it. 

# Big Data Analysis Processing Pipeline 
<img src="https://github.com/akhil2607/akhil2607/assets/44285279/fff4a540-a3c1-4c75-8606-c2276ddc9af2" width="700" height="400">

## Phases of Big Data Analysis

### 1. Data Acquisition and Recording :
In this part, facts from some made-up sources are written down. A lot of material is useless, but it can be organized and cut down by a whole lot. There were two big issues in this first part. First, the data filter can be hard to describe because it has to pick out sources or information that are helpful and get rid of the ones that aren't. Second, it's hard to get information right away about what data is being recorded and how it is being tracked and recorded. This also means that the material will have its own past. In this way, the model can fix any processing errors that happen during the image and analysis process. It's also better at dealing with noises outside the model. 

### 2. Information Extraction and Cleaning :
We are missing a lot of info. It's not clear where to find the X-ray picture data and the chat tape record. It should be possible to pick out and organize useful data for research. It's calling out facts. This always happens with real-world tools, which is a shame. Data may be hidden or skewed. It looks like predictions are rarely right, and we can't change that. The way a hospital is set up can change data counts and trends, which can cause problems like wrong labels and other issues. Patients may lie about their goals from time to time. However, these traits don't seem to be common in many Big Data areas. It's very important to have clear mistake models and limits on how much info is true. 

### 3. Data Integration, Aggregation and Representation :
This step makes sure that the information is correct and that the data can be used again within a planned data structure. It also makes sure that the meanings are expressed in a way that computers can understand. It will also be possible to solve it automatically, though extra work may be needed to avoid mistakes. Database creation is an art form now. To make databases that work well even when there isn't clever database design, more people from different areas need to be able to use design methods or tools. 

### 4. Query Processing, Data Modelling and Analysis :
Big Data questioning and mining differs from small-group statistics. Big Data changes, is linked, messy, and unreliable. Big Data may disclose precise hidden patterns and information despite its confusion. It also mitigates slight sample mistakes and individual alterations. Big Data coupled together may generate large information networks. Data mining and clever queries may improve data quality and reliability. The next generation of live data analysis can obtain real-time responses owing to Big Data. However, system integration remains a major issue in many settings. 

### 5. Interpretation :
It means that the person can understand and make sense of the results of the data analysis. To understand and analyze the data analysis processes, you need to know the key concepts that the data is based on. You can help explain the result and see the trends in the data by adding extra details like the data's surroundings or context. 

## Challenges of Big Data Analysis

### 1. Heterogeneity and Incompleteness :
For example, In the healthcare field, for example, a patient may have more than one set of lab reports and medical records. Before data can be analyzed, it needs to be carefully organized. In the "right" step, the right questions need to be asked. Before you start analyzing the data, you should also carefully look at the records that are missing. These records could be explained or left out of the database. You might still find noise in the data even after you clean it up and fix any mistakes. 

### 2. Scale : 
The sale of Big Data is one of the most important things to think about. Along with the huge growth in the amount of Big Data we receive, the fast development of chip technology and cloud computing has brought up a lot of problems and unknowns. The new way of working with data design and system-driven overall optimization may make the current choices more or less useless.

### 3. Speed :
The speed of processing. Structures and queries are directly related to the amount of time required to process a large amount of data. In many cases, the study report needs to be given right away. When you have a big set of data, you often need to find parts of it that meet certain standards. It's usually a question or search issue. Based on what the collection is like, we need new index structures.

### 4. Data Privacy :
Privacy of one's data is quickly becoming one of today's most pressing problems. The multifaceted nature of big data ensures that it will continue to be a tough and contentious subject of conversation, despite the possibility of their being codified laws that limit what can be done and make it plain what cannot. There are also a range of extra difficult research problems, such as restricted disclosure restrictions and user rights.

### 5. Human Collaboration :
Human collaboration can bring a hard time for computer, due to the multidisciplinary team structure and the crowd-sourcing reality. For Big Data analytics to work best, they should include both computer analysis and human analysis. Having a person in the loop. For Big Data research to work, it needs to be possible for different human experts to give input and for everyone to look at the results together. 
Crowdsourcing sites like Wikipedia, Yelp reviews, and Amazon Mechanical Turk can cause disagreement, doubt, and mistakes. We need to be able to handle these issues.


# References

- What is Big Data? (n.d.). Oracle. https://www.oracle.com/big-data/what-is-big-data/
- GeeksforGeeks. (2023). Types of big data. GeeksforGeeks. https://www.geeksforgeeks.org/types-of-big-data/
- GeeksforGeeks. (2023b). 6V s of Big Data. GeeksforGeeks. https://www.geeksforgeeks.org/5-vs-of-big-data/
- Zhao, C. (n.d.). Five Phases in the Big Data Processing Pipeline (notes). www.linkedin.com. https://www.linkedin.com/pulse/five-phases-big-data-processing-pipeline-notes-christiane-zhao
